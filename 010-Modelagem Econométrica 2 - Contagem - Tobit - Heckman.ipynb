{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c31e812",
   "metadata": {},
   "source": [
    "# Modelos de Contagem, Tobit e Seleção Amostral\n",
    "\n",
    "Prof. Daniel de Abreu Pereira Uhr\n",
    "\n",
    "### Conteúdo\n",
    "\n",
    "* Modelos de Contagem\n",
    "  * Poisson Model\n",
    "  * Negative Binomial Model\n",
    "  * Zero-Inflated Poisson (ZIP)\n",
    "  * Zero Inflated Negative Binomial Model (ZINB)\n",
    "* Modelo Tobit\n",
    "* Modelo de Seleção Amostral\n",
    "\n",
    "\n",
    "**Referências**\n",
    "\n",
    "* Cameron e Triverdi – Microeconometrics using stata. \n",
    "  * Capítulo 16 – Tobit and Selection models\n",
    "  * Capítulo 17 – Count Data Models\n",
    "* Cameron e Triverdi – Microeconometrics, Methods and Aplication\n",
    "  * Capítulo 16 – Tobit and Selection models\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c293eae",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b008fa",
   "metadata": {},
   "source": [
    "## Modelos de Contagem\n",
    "\n",
    "### Modelo de Poisson\n",
    "\n",
    "Em muitos contextos de pesquisa, a variável de resultado $Y$ é uma contagem, ou seja, um número inteiro não negativo. Exemplos comuns incluem o número de chamadas recebidas por um call center, o número de acidentes de trânsito em uma determinada interseção, ou o número de filhos em uma família. Nesses casos, a variável $Y$ pertence ao conjunto $\\mathbb{N}_0 = \\{0, 1, 2, 3, \\ldots \\}$. O objetivo é modelar $Y$ como uma função das variáveis explicativas $X$ através de uma estrutura de regressão.\n",
    "\n",
    "Uma modelagem natural para esses dados de contagem é o **Modelo de Poisson**, que assume que o número de ocorrências de um evento segue uma distribuição de Poisson.\n",
    "\n",
    "#### Função de Probabilidade da Distribuição de Poisson\n",
    "\n",
    "A distribuição de Poisson é definida para modelar a probabilidade do número de ocorrências de um evento em um intervalo fixo de tempo ou espaço, dado que esses eventos ocorrem com uma taxa média constante e independentemente do tempo desde o último evento. A função de probabilidade para $Y$, o número de ocorrências, é dada por:\n",
    "\n",
    "$$ \\Pr(Y = y) = \\frac{\\lambda^y e^{-\\lambda}}{y!}, \\quad y = 0, 1, 2, \\ldots $$\n",
    "\n",
    "Onde:\n",
    "- $\\lambda$ é o parâmetro da distribuição, que representa tanto a média quanto a variância do número de ocorrências.\n",
    "- $e$ é a base do logaritmo natural (aproximadamente 2,718).\n",
    "- $y!$ é o fatorial de $y$, ou seja, o produto de todos os inteiros positivos até $y$.\n",
    "\n",
    "#### Momentos da Distribuição de Poisson\n",
    "\n",
    "Os momentos da distribuição de Poisson são dados por:\n",
    "\n",
    "- **Esperança (Média):**\n",
    "  $$ E(Y) = \\lambda $$\n",
    "\n",
    "- **Variância:**\n",
    "  $$ \\text{Var}(Y) = \\lambda $$\n",
    "\n",
    "Um aspecto notável da distribuição de Poisson é que a média e a variância são iguais. Isso contrasta com muitos modelos de regressão tradicionais, onde a variância é assumida constante.\n",
    "\n",
    "#### Parametrização do Modelo de Poisson\n",
    "\n",
    "No contexto de regressão, queremos modelar o parâmetro $\\lambda$ como uma função das variáveis explicativas $X$. Para garantir que $\\lambda$ seja sempre positivo (já que é uma taxa de ocorrência), utilizamos a função exponencial:\n",
    "\n",
    "$$ \\lambda = \\exp(x' \\beta) $$\n",
    "\n",
    "Onde:\n",
    "- $x'$ é o vetor transposto das variáveis explicativas.\n",
    "- $\\beta$ é o vetor de coeficientes a serem estimados.\n",
    "\n",
    "Assim, a esperança condicional de $Y$ dado $X$ é:\n",
    "\n",
    "$$ E(Y|X) = \\lambda = \\exp(x' \\beta) $$\n",
    "\n",
    "E, como a variância também é igual a $\\lambda$:\n",
    "\n",
    "$$ \\text{Var}(Y|X) = \\lambda = \\exp(x' \\beta) $$\n",
    "\n",
    "#### Interpretação dos Coeficientes no Modelo de Poisson\n",
    "\n",
    "Os coeficientes $\\beta_j$ no modelo de Poisson têm uma interpretação multiplicativa. Mais especificamente, o exponencial de $\\beta_j$ representa a mudança proporcional na média esperada de $Y$ para um aumento unitário em $x_j$, mantendo as outras variáveis constantes.\n",
    "\n",
    "Por exemplo, se $\\beta_j = 0.2$, então $\\exp(0.2) \\approx 1.22$, o que indica que um aumento unitário em $x_j$ está associado a um aumento de 22% na média esperada de $Y$.\n",
    "\n",
    "#### Heterocedasticidade no Modelo de Poisson\n",
    "\n",
    "O modelo de Poisson é **intrinsecamente heterocedástico**, o que significa que a variância de $Y$ não é constante, mas varia com o valor esperado $\\lambda$. À medida que $\\lambda$ aumenta (por exemplo, conforme aumentam as variáveis $X$), a variância de $Y$ também aumenta. Isso contrasta com os modelos de regressão linear clássicos, que assumem homocedasticidade (variância constante dos erros).\n",
    "\n",
    "#### Exemplo Prático de Aplicação do Modelo de Poisson\n",
    "\n",
    "Suponha que estamos interessados em modelar o número de chamadas recebidas por um call center em um dia típico. As variáveis explicativas podem incluir o número de agentes disponíveis, o dia da semana e a época do ano. Usando o modelo de Poisson, podemos estimar a relação entre essas variáveis e o número esperado de chamadas.\n",
    "\n",
    "Se o coeficiente estimado para o número de agentes disponíveis for positivo, isso indicaria que, conforme o número de agentes aumenta, esperamos um maior número de chamadas recebidas, refletido no aumento do parâmetro $\\lambda$.\n",
    "\n",
    "#### Limitações do Modelo de Poisson\n",
    "\n",
    "Embora o modelo de Poisson seja útil em muitas situações, ele tem algumas limitações:\n",
    "\n",
    "1. **Superdispersão**: O modelo de Poisson assume que a média e a variância são iguais. No entanto, em muitos conjuntos de dados reais, a variância é maior do que a média, um fenômeno conhecido como superdispersão. Quando há superdispersão, o modelo de Poisson pode subestimar a variabilidade dos dados, levando a erros-padrão subestimados e testes de hipóteses inválidos.\n",
    "\n",
    "2. **Subdispersão**: Menos comum que a superdispersão, mas ocorre quando a variância é menor que a média, o que também não é capturado adequadamente pelo modelo de Poisson.\n",
    "\n",
    "#### Alternativas ao Modelo de Poisson\n",
    "\n",
    "Quando a suposição de equidispersão (igualdade entre média e variância) não é adequada, outras abordagens podem ser utilizadas, como:\n",
    "\n",
    "- **Modelo Binomial Negativo**: Extensão do modelo de Poisson que permite que a variância seja maior que a média.\n",
    "- **Modelos de Poisson Hurdle e Zero-Inflated Poisson (ZIP)**: Usados para lidar com um excesso de zeros nas contagens, comum em muitos conjuntos de dados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9da07731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7145d7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame\n",
    "df = pd.read_stata(\"https://github.com/Daniel-Uhr/data/raw/main/cattaneo2.dta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5a34268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustando as variáveis\n",
    "# Criar a variável de resultado de contagem\n",
    "df['Y'] = df['nprenatal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdbfa23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 2.765307\n",
      "         Iterations 5\n",
      "                          Poisson Regression Results                          \n",
      "==============================================================================\n",
      "Dep. Variable:                      Y   No. Observations:                 4642\n",
      "Model:                        Poisson   Df Residuals:                     4636\n",
      "Method:                           MLE   Df Model:                            5\n",
      "Date:                Thu, 08 Aug 2024   Pseudo R-squ.:                 0.01619\n",
      "Time:                        16:05:31   Log-Likelihood:                -12837.\n",
      "converged:                       True   LL-Null:                       -13048.\n",
      "Covariance Type:            nonrobust   LLR p-value:                 4.108e-89\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      1.8943      0.027     69.755      0.000       1.841       1.947\n",
      "medu           0.0151      0.002      6.552      0.000       0.011       0.020\n",
      "fedu           0.0127      0.002      8.117      0.000       0.010       0.016\n",
      "mage           0.0052      0.001      5.692      0.000       0.003       0.007\n",
      "deadkids      -0.0201      0.010     -1.930      0.054      -0.041       0.000\n",
      "alcohol       -0.1130      0.027     -4.190      0.000      -0.166      -0.060\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Definindo a fórmula da regressão de Poisson\n",
    "formula = 'Y ~ medu + fedu + mage + medu + deadkids + alcohol'\n",
    "\n",
    "# Ajustando o modelo de Poisson\n",
    "poisson_model = smf.poisson(formula=formula, data=df).fit()\n",
    "\n",
    "# Exibindo o resumo do modelo\n",
    "print(poisson_model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32571460",
   "metadata": {},
   "source": [
    "Para calcular os efeitos marginais, temos duas opções:\n",
    "* **Overall**: calcula os efeitos marginais para cada observação individualmente e, em seguida, tira a média desses efeitos marginais. Isso dá uma média dos efeitos marginais observados na amostra.\n",
    "* **Mean**: fixa todas as covariáveis em seus valores médios. Isto é, o efeito marginal é calculado assumindo que todas as variáveis independentes estão em seus valores médios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5136eddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Poisson Marginal Effects      \n",
      "=====================================\n",
      "Dep. Variable:                      Y\n",
      "Method:                          dydx\n",
      "At:                           overall\n",
      "==============================================================================\n",
      "                dy/dx    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "medu           0.1629      0.025      6.549      0.000       0.114       0.212\n",
      "fedu           0.1371      0.017      8.111      0.000       0.104       0.170\n",
      "mage           0.0556      0.010      5.690      0.000       0.036       0.075\n",
      "deadkids      -0.2163      0.112     -1.930      0.054      -0.436       0.003\n",
      "alcohol       -1.2157      0.290     -4.189      0.000      -1.784      -0.647\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculando os efeitos marginais\n",
    "marginal_effects1 = poisson_model.get_margeff()\n",
    "\n",
    "# Exibindo o resumo dos efeitos marginais\n",
    "print(marginal_effects1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1136acbe",
   "metadata": {},
   "source": [
    "Os efeitos marginais fornecem uma interpretação adicional dos coeficientes do modelo de Poisson, mostrando como a variável dependente média muda com uma mudança unitária nas variáveis independentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e51a041b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Poisson Marginal Effects      \n",
      "=====================================\n",
      "Dep. Variable:                      Y\n",
      "Method:                          dydx\n",
      "At:                              mean\n",
      "==============================================================================\n",
      "                dy/dx    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "medu           0.1622      0.025      6.555      0.000       0.114       0.211\n",
      "fedu           0.1365      0.017      8.123      0.000       0.104       0.169\n",
      "mage           0.0554      0.010      5.694      0.000       0.036       0.074\n",
      "deadkids      -0.2153      0.112     -1.930      0.054      -0.434       0.003\n",
      "alcohol       -1.2104      0.289     -4.191      0.000      -1.776      -0.644\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "marginal_effects2 = poisson_model.get_margeff(at='mean')\n",
    "print(marginal_effects2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8366fa",
   "metadata": {},
   "source": [
    "### Sobredispersão no Modelo de Poisson\n",
    "\n",
    "Uma limitação importante do modelo de Poisson é a suposição de que a média e a variância da variável dependente são iguais, o que é conhecido como **equidispersão**. No entanto, em muitos conjuntos de dados reais, a variância dos dados é maior do que a média, um fenômeno conhecido como **sobredispersão**. Quando há sobredispersão, o modelo de Poisson padrão pode produzir estimativas inadequadas, subestimando a variabilidade dos dados, o que leva a erros-padrão subestimados e, consequentemente, a testes de hipóteses inválidos.\n",
    "\n",
    "#### Teste de Sobredispersão\n",
    "\n",
    "Após estimar uma regressão de Poisson, é essencial testar se há sobredispersão nos dados. Um teste comum para isso envolve verificar se a variância condicional $ \\text{Var}(y|x) $ é maior do que a média condicional $ \\mu_i $. A forma especificada da sobredispersão pode ser expressa da seguinte maneira:\n",
    "\n",
    "$$ \\text{Var}(y|x) = \\mu_i + \\alpha g(\\mu_i) $$\n",
    "\n",
    "Onde:\n",
    "- $ \\alpha $ é um parâmetro desconhecido que captura a magnitude da sobredispersão.\n",
    "- $ g(\\cdot) $ é uma função conhecida, que muitas vezes assume a forma $ g(\\mu_i) = \\mu_i^2$.\n",
    "\n",
    "#### Hipóteses do Teste:\n",
    "\n",
    "- **Hipótese nula (H0):** $ \\alpha = 0 $, o que implica que $ \\text{Var}(y|x) = \\mu_i $. Ou seja, não há sobredispersão, e o modelo de Poisson é apropriado.\n",
    "- **Hipótese alternativa (H1):** $ \\alpha \\ne 0 $, indicando que $ \\text{Var}(y|x) $ excede $ \\mu_i $, sugerindo a presença de sobredispersão.\n",
    "\n",
    "Se o teste indicar a presença de sobredispersão (ou seja, $ \\alpha $ é significativamente diferente de zero), o modelo de Poisson padrão não será adequado. Neste caso, pode-se recorrer a:\n",
    "\n",
    "1. **Modelo de Poisson robusto à heterocedasticidade**: Ajusta os erros-padrão para lidar com a variabilidade excessiva.\n",
    "2. **Modelo Binomial Negativo**: Um modelo alternativo que relaxa a suposição de equidispersão, permitindo que a variância seja maior que a média.\n",
    "\n",
    "#### Implementação do Teste de Sobredispersão\n",
    "\n",
    "Abaixo está um exemplo em Python para calcular a estatística de sobredispersão usando os resíduos de Pearson após ajustar um modelo de Poisson:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "289f2315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estatística de sobredispersão: 1.2272192020835233\n",
      "p-valor: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Calculando os resíduos Pearson\n",
    "pearson_residuals = poisson_model.resid_pearson\n",
    "\n",
    "# Calculando a estatística de sobredispersão\n",
    "dispersion_statistic = np.sum(pearson_residuals**2) / poisson_model.df_resid\n",
    "\n",
    "# Calculando o p-valor\n",
    "p_value = 1 - chi2.cdf(dispersion_statistic, df=poisson_model.df_resid)\n",
    "\n",
    "print(f\"Estatística de sobredispersão: {dispersion_statistic}\")\n",
    "print(f\"p-valor: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3f5e9b",
   "metadata": {},
   "source": [
    "### Modelo Binomial Negativo\n",
    "\n",
    "Quando trabalhamos com variáveis dependentes de contagem não negativa, é comum utilizar o modelo de Poisson para modelar a relação entre a variável resposta e as variáveis explicativas. No entanto, uma limitação significativa do modelo de Poisson é a suposição de equidispersão, onde a variância da contagem é igual à média. Em muitos casos reais, a variância excede a média, uma situação conhecida como **sobredispersão**. Para lidar com a sobredispersão, utilizamos o **Modelo Binomial Negativo**.\n",
    "\n",
    "#### Momentos da Distribuição Binomial Negativa\n",
    "\n",
    "Os primeiros dois momentos da distribuição binomial negativa são:\n",
    "\n",
    "$$ E(y|x) = \\mu $$\n",
    "\n",
    "$$ \\text{Var}(y|x) = \\mu(1 + \\alpha \\mu) $$\n",
    "\n",
    "Aqui, a variância excede a média, indicando que $\\alpha > 0$ e $\\mu > 0$. A sobredispersão surge porque, ao contrário do modelo de Poisson, há uma heterogeneidade não observada que afeta a contagem de eventos. Essa heterogeneidade pode ser modelada como uma variação multiplicativa no parâmetro $\\lambda$ do modelo de Poisson.\n",
    "\n",
    "#### Heterogeneidade Não Observada\n",
    "\n",
    "A sobredispersão pode ser modelada assumindo que a contagem $y$ segue uma distribuição de Poisson condicional a uma variável latente de heterogeneidade $v$. Se a variável latente $v$ segue uma distribuição gamma, a contagem $y$ tem uma distribuição binomial negativa marginal:\n",
    "\n",
    "$$ y_j \\sim \\text{Poisson}(\\mu_j) $$\n",
    "\n",
    "$$ \\mu_j = \\exp(x_j' \\beta + \\text{offset}_j + v_j) = \\exp(x_j' \\beta + \\text{offset}_j) \\cdot \\exp(v_j) $$\n",
    "\n",
    "Onde:\n",
    "- **Offset**: Um parâmetro de ajuste associado à exposição (como tempo ou área).\n",
    "- $e^{v_j} \\sim \\text{Gamma}(1/\\alpha, \\alpha)$: Aqui, a distribuição gamma modela a variabilidade não observada, com $\\alpha$ sendo o parâmetro de sobredispersão.\n",
    "\n",
    "Na parametrização gamma, a distribuição $\\text{Gamma}(a, b)$ tem esperança $a \\cdot b$ e variância $a \\cdot b^2$. Quando $\\alpha = 0$, o modelo binomial negativo se reduz ao modelo de Poisson.\n",
    "\n",
    "#### Parametrizações NB1 e NB2\n",
    "\n",
    "No modelo binomial negativo, podemos ajustar diferentes parametrizações da variância:\n",
    "\n",
    "- **NB1**: Assume que a variância é constante para todas as observações, dada por $\\text{Var}(y_j) = \\mu(1 + \\delta)$. Este modelo é útil em contextos onde a dispersão é constante.\n",
    "  \n",
    "- **NB2**: Assume que a variância é uma função quadrática da média, dada por $\\text{Var}(y_j) = \\mu(1 + \\alpha \\mu)$. Este modelo é mais flexível e captura melhor a variação da dispersão com a média, sendo a parametrização mais comum.\n",
    "\n",
    "#### Modelos Generalizados de Binomial Negativo\n",
    "\n",
    "No modelo binomial negativo generalizado, o parâmetro $\\alpha$ pode ser modelado como uma função das covariáveis, permitindo que a sobredispersão varie entre diferentes grupos ou condições. Especificamente, podemos modelar $\\ln(\\alpha)$ como uma função linear das covariáveis, proporcionando maior flexibilidade no ajuste dos dados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "41d3988f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 2.753363\n",
      "         Iterations: 18\n",
      "         Function evaluations: 26\n",
      "         Gradient evaluations: 26\n",
      "\n",
      "Modelo Binomial Negativa com Dispersão Constante (NB1):\n",
      "                     NegativeBinomial Regression Results                      \n",
      "==============================================================================\n",
      "Dep. Variable:                      Y   No. Observations:                 4642\n",
      "Model:               NegativeBinomial   Df Residuals:                     4636\n",
      "Method:                           MLE   Df Model:                            5\n",
      "Date:                qua, 07 ago 2024   Pseudo R-squ.:                 0.01462\n",
      "Time:                        14:58:37   Log-Likelihood:                -12781.\n",
      "converged:                       True   LL-Null:                       -12971.\n",
      "Covariance Type:            nonrobust   LLR p-value:                 8.342e-80\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      1.8691      0.030     61.508      0.000       1.810       1.929\n",
      "medu           0.0155      0.003      5.997      0.000       0.010       0.021\n",
      "fedu           0.0139      0.002      7.836      0.000       0.010       0.017\n",
      "mage           0.0054      0.001      5.355      0.000       0.003       0.007\n",
      "deadkids      -0.0224      0.012     -1.922      0.055      -0.045       0.000\n",
      "alcohol       -0.1307      0.031     -4.267      0.000      -0.191      -0.071\n",
      "alpha          0.2498      0.028      8.997      0.000       0.195       0.304\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Ajustando o modelo Binomial Negativa com dispersão constante (NB1)\n",
    "nb1_model = smf.negativebinomial(formula=formula, data=df, loglike_method='nb1').fit()\n",
    "\n",
    "print(\"\\nModelo Binomial Negativa com Dispersão Constante (NB1):\")\n",
    "print(nb1_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "42b19056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 2.757636\n",
      "         Iterations: 21\n",
      "         Function evaluations: 24\n",
      "         Gradient evaluations: 24\n",
      "\n",
      "Modelo Binomial Negativa com Dispersão Média (NB2):\n",
      "                     NegativeBinomial Regression Results                      \n",
      "==============================================================================\n",
      "Dep. Variable:                      Y   No. Observations:                 4642\n",
      "Model:               NegativeBinomial   Df Residuals:                     4636\n",
      "Method:                           MLE   Df Model:                            5\n",
      "Date:                qua, 07 ago 2024   Pseudo R-squ.:                 0.01309\n",
      "Time:                        14:58:40   Log-Likelihood:                -12801.\n",
      "converged:                       True   LL-Null:                       -12971.\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.922e-71\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      1.8913      0.030     63.493      0.000       1.833       1.950\n",
      "medu           0.0153      0.003      6.056      0.000       0.010       0.020\n",
      "fedu           0.0128      0.002      7.469      0.000       0.009       0.016\n",
      "mage           0.0052      0.001      5.211      0.000       0.003       0.007\n",
      "deadkids      -0.0201      0.011     -1.763      0.078      -0.043       0.002\n",
      "alcohol       -0.1131      0.029     -3.859      0.000      -0.171      -0.056\n",
      "alpha          0.0189      0.003      7.426      0.000       0.014       0.024\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Ajustando o modelo Binomial Negativa com dispersão média (NB2)\n",
    "nb2_model = smf.negativebinomial(formula=formula, data=df, loglike_method='nb2').fit()\n",
    "\n",
    "print(\"\\nModelo Binomial Negativa com Dispersão Média (NB2):\")\n",
    "print(nb2_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6b68fac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Poisson AIC: 25685.106583294248\n",
      "NB1 AIC (Dispersão Constante): 25576.217835521515\n",
      "NB2 AIC (Dispersão Média): 25615.895551374393\n"
     ]
    }
   ],
   "source": [
    "# Comparando os modelos usando o AIC\n",
    "print(f\"\\nPoisson AIC: {poisson_model.aic}\")\n",
    "print(f\"NB1 AIC (Dispersão Constante): {nb1_model.aic}\")\n",
    "print(f\"NB2 AIC (Dispersão Média): {nb2_model.aic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce228b52",
   "metadata": {},
   "source": [
    "Um AIC mais baixo indica um melhor ajuste do modelo. além disso, A Binomial Negativa é um modelo que ajusta essa sobredispersão introduzindo o parâmetro $\\alpha$. Um valor positivo de $\\alpha$ indica que há sobredispersão nos dados. Quanto maior o valor de $\\alpha$, maior a discrepância entre a variância e a média da variável dependente, indicando maior sobredispersão.\n",
    "Este valor positivo de $\\alpha$ sugere que há sobredispersão nos dados. A variância da variável dependente é maior do que a média, e a Binomial Negativa está ajustando isso melhor do que um modelo de Poisson simples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cb9b8e",
   "metadata": {},
   "source": [
    "### Zero-Inflated Poisson (ZIP)\n",
    "\n",
    "O **Modelo Zero-Inflated Poisson (ZIP)** é uma extensão do modelo de Poisson que é particularmente útil quando os dados de contagem apresentam um número excessivo de zeros, ou seja, quando há mais zeros nos dados do que seria esperado sob uma distribuição de Poisson padrão. O modelo ZIP combina dois processos: um processo que gera apenas zeros e um processo que gera contagens de acordo com uma distribuição de Poisson.\n",
    "\n",
    "### Estrutura do Modelo ZIP\n",
    "\n",
    "O modelo ZIP considera que a variável dependente $Y$ pode ser gerada por dois estados distintos:\n",
    "\n",
    "1. **Estado Zero-Inflacionado**: Um estado em que a única possível observação é zero. Esse estado é modelado por uma distribuição Bernoulli.\n",
    "2. **Estado Poisson**: Um estado em que a contagem é gerada de acordo com uma distribuição de Poisson tradicional.\n",
    "\n",
    "### Componentes do Modelo ZIP\n",
    "\n",
    "- **Probabilidade de Zero-Inflacionamento** ($\\pi$): Representa a probabilidade de que a observação esteja no estado zero-inflacionado.\n",
    "- **Distribuição Poisson** ($\\lambda$): Representa a taxa média de ocorrência de eventos na distribuição Poisson, caso a observação não esteja no estado zero-inflacionado.\n",
    "\n",
    "A função de probabilidade do modelo ZIP é dada por:\n",
    "\n",
    "$$\n",
    "\\Pr(Y = y) = \n",
    "\\begin{cases} \n",
    "\\pi + (1 - \\pi) \\cdot \\exp(-\\lambda), & \\text{se } y = 0 \\\\\n",
    "(1 - \\pi) \\cdot \\frac{\\lambda^y \\exp(-\\lambda)}{y!}, & \\text{se } y > 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Onde:\n",
    "- $\\pi$ é a probabilidade de observar um zero inflacionado (estado 1).\n",
    "- $\\lambda$ é o parâmetro da distribuição Poisson no estado 2.\n",
    "\n",
    "### Interpretação dos Componentes\n",
    "\n",
    "- **$\\pi$**: Modela a probabilidade de ocorrência de um zero excessivo, que não é capturado pela distribuição de Poisson.\n",
    "- **$\\lambda$**: Modela a taxa de contagem para a distribuição Poisson quando a observação não está no estado zero-inflacionado.\n",
    "\n",
    "### Estimação do Modelo ZIP\n",
    "\n",
    "A estimação do modelo ZIP envolve a maximização da função de verossimilhança para ambos os componentes simultaneamente. Os parâmetros $\\pi$ e $\\lambda$ podem ser modelados como funções das covariáveis $X$:\n",
    "\n",
    "- **$\\pi$**: A probabilidade de inflacionamento de zeros pode ser modelada como uma função logística das covariáveis: \n",
    "  $$\n",
    "  \\pi = \\frac{\\exp(z' \\gamma)}{1 + \\exp(z' \\gamma)}\n",
    "  $$\n",
    "  Onde $z$ são as covariáveis que afetam a probabilidade de inflacionamento e $\\gamma$ são os coeficientes a serem estimados.\n",
    "\n",
    "- **$\\lambda$**: A média da distribuição de Poisson pode ser modelada como uma função exponencial das covariáveis:\n",
    "  $$\n",
    "  \\lambda = \\exp(x' \\beta)\n",
    "  $$\n",
    "  Onde $x$ são as covariáveis que afetam a contagem e $\\beta$ são os coeficientes a serem estimados.\n",
    "\n",
    "### Aplicações Típicas do Modelo ZIP\n",
    "\n",
    "O modelo ZIP é utilizado em situações onde há muitos zeros e as contagens positivas podem ser modeladas por uma distribuição de Poisson. Exemplos incluem:\n",
    "\n",
    "- **Número de visitas ao médico**: Alguns indivíduos podem nunca visitar o médico, enquanto outros o fazem regularmente, o que cria um excesso de zeros.\n",
    "- **Número de acidentes em um cruzamento**: Muitos cruzamentos podem não ter acidentes em um determinado período, mas aqueles que têm acidentes podem seguir uma distribuição de Poisson.\n",
    "- **Número de compras online**: Alguns usuários podem nunca realizar compras online, resultando em zeros inflacionados.\n",
    "\n",
    "### Limitações do Modelo ZIP\n",
    "\n",
    "Apesar de sua utilidade, o modelo ZIP tem algumas limitações:\n",
    "\n",
    "1. **Assunção de Independência**: O modelo assume que os zeros no estado zero-inflacionado são gerados de maneira independente das contagens no estado Poisson, o que pode não ser realista em todos os contextos.\n",
    "2. **Complexidade na Interpretação**: Interpretar os resultados de um modelo ZIP pode ser mais complexo devido à mistura de dois processos subjacentes.\n",
    "\n",
    "### Alternativas ao Modelo ZIP\n",
    "\n",
    "Se o modelo ZIP não for adequado, outras alternativas podem ser consideradas:\n",
    "\n",
    "- **Modelo Zero-Inflated Binomial Negativo (ZINB)**: Uma extensão do modelo ZIP que é útil quando há sobredispersão adicional nos dados além do excesso de zeros.\n",
    "- **Modelo Hurdle**: Um modelo que separa o processo de geração de zeros do processo que gera contagens positivas, mas de forma diferente do ZIP.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f70b1e",
   "metadata": {},
   "source": [
    "### Zero Inflated Negative Binomial Model (ZINB)\n",
    "\n",
    "O **Modelo Zero-Inflated Negative Binomial (ZINB)** é uma extensão do modelo Zero-Inflated Poisson (ZIP) que combina a modelagem de contagens com a capacidade de lidar tanto com o excesso de zeros quanto com a sobredispersão. Enquanto o modelo ZIP assume que as contagens positivas seguem uma distribuição de Poisson, o modelo ZINB assume que essas contagens seguem uma distribuição binomial negativa, o que permite uma maior flexibilidade ao modelar dados em que a variância é maior que a média.\n",
    "\n",
    "### Estrutura do Modelo ZINB\n",
    "\n",
    "Assim como o modelo ZIP, o modelo ZINB considera que a variável dependente $Y$ pode ser gerada por dois processos distintos:\n",
    "\n",
    "1. **Estado Zero-Inflacionado**: Um estado em que a única possível observação é zero. Esse estado é modelado por uma distribuição Bernoulli.\n",
    "2. **Estado Binomial Negativo**: Um estado em que a contagem é gerada de acordo com uma distribuição binomial negativa, permitindo que a variância das contagens seja maior que a média (sobredispersão).\n",
    "\n",
    "### Componentes do Modelo ZINB\n",
    "\n",
    "- **Probabilidade de Zero-Inflacionamento** ($\\pi$): Representa a probabilidade de que a observação esteja no estado zero-inflacionado.\n",
    "- **Distribuição Binomial Negativa** ($\\lambda$): Representa a taxa média de ocorrência de eventos na distribuição binomial negativa, caso a observação não esteja no estado zero-inflacionado.\n",
    "\n",
    "A função de probabilidade do modelo ZINB é dada por:\n",
    "\n",
    "$$\n",
    "\\Pr(Y = y) = \n",
    "\\begin{cases} \n",
    "\\pi + (1 - \\pi) \\cdot \\text{NB}(0|\\lambda, \\alpha), & \\text{se } y = 0 \\\\\n",
    "(1 - \\pi) \\cdot \\text{NB}(y|\\lambda, \\alpha), & \\text{se } y > 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Onde:\n",
    "- $\\pi$ é a probabilidade de observar um zero inflacionado (estado 1).\n",
    "- $\\lambda$ é o parâmetro da distribuição binomial negativa no estado 2.\n",
    "- $\\alpha$ é o parâmetro de dispersão da distribuição binomial negativa, que controla o grau de sobredispersão.\n",
    "\n",
    "### Interpretação dos Componentes\n",
    "\n",
    "- **$\\pi$**: Modela a probabilidade de ocorrência de um zero inflacionado, que não é capturado pela distribuição binomial negativa.\n",
    "- **$\\lambda$**: Modela a taxa de contagem para a distribuição binomial negativa quando a observação não está no estado zero-inflacionado.\n",
    "- **$\\alpha$**: Controla a sobredispersão nas contagens positivas. Se $\\alpha = 0$, o modelo ZINB se reduz ao modelo ZIP.\n",
    "\n",
    "### Estimação do Modelo ZINB\n",
    "\n",
    "A estimação do modelo ZINB envolve a maximização da função de verossimilhança para os dois processos (zero-inflacionado e binomial negativo) simultaneamente. Assim como no modelo ZIP, os parâmetros $\\pi$ e $\\lambda$ podem ser modelados como funções das covariáveis $X$:\n",
    "\n",
    "- **$\\pi$**: A probabilidade de inflacionamento de zeros pode ser modelada como uma função logística das covariáveis:\n",
    "  $$\n",
    "  \\pi = \\frac{\\exp(z' \\gamma)}{1 + \\exp(z' \\gamma)}\n",
    "  $$\n",
    "  Onde $z$ são as covariáveis que afetam a probabilidade de inflacionamento e $\\gamma$ são os coeficientes a serem estimados.\n",
    "\n",
    "- **$\\lambda$**: A média da distribuição binomial negativa pode ser modelada como uma função exponencial das covariáveis:\n",
    "  $$\n",
    "  \\lambda = \\exp(x' \\beta)\n",
    "  $$\n",
    "  Onde $x$ são as covariáveis que afetam a contagem e $\\beta$ são os coeficientes a serem estimados.\n",
    "\n",
    "### Aplicações Típicas do Modelo ZINB\n",
    "\n",
    "O modelo ZINB é particularmente útil em situações onde há um excesso de zeros e a variância das contagens positivas é maior que a média. Exemplos incluem:\n",
    "\n",
    "- **Número de visitas a um serviço de saúde**: Alguns indivíduos podem nunca visitar um médico, enquanto outros o fazem frequentemente, com variância alta no número de visitas.\n",
    "- **Incidência de crimes em regiões específicas**: Algumas regiões podem não ter nenhum registro de crime, enquanto outras têm registros frequentes, mas com grande variabilidade.\n",
    "- **Contagem de espécies em estudos ecológicos**: Algumas áreas podem não conter certas espécies, enquanto outras áreas têm grande variação na contagem dessas espécies.\n",
    "\n",
    "### Limitações do Modelo ZINB\n",
    "\n",
    "Apesar de sua flexibilidade, o modelo ZINB possui algumas limitações:\n",
    "\n",
    "1. **Complexidade Computacional**: A estimação dos parâmetros no modelo ZINB pode ser mais complexa e exigente em termos computacionais do que em modelos mais simples, como o ZIP ou o modelo de Poisson.\n",
    "2. **Interpretação Complexa**: A interpretação dos coeficientes e a separação dos efeitos entre os dois processos (zero-inflacionado e binomial negativo) pode ser desafiadora.\n",
    "3. **Necessidade de Dados Ricos**: Para que o modelo ZINB seja efetivo, é necessário um conjunto de dados suficientemente grande e rico em variabilidade, especialmente para identificar corretamente os parâmetros de sobredispersão.\n",
    "\n",
    "### Alternativas ao Modelo ZINB\n",
    "\n",
    "Se o modelo ZINB não for adequado, outras alternativas podem ser consideradas:\n",
    "\n",
    "- **Modelo Hurdle**: Um modelo que também lida com zeros excessivos, mas trata a geração de zeros e contagens positivas como dois processos separados e independentes.\n",
    "- **Modelo Zero-Inflated Poisson (ZIP)**: Utilizado quando a sobredispersão não é um problema significativo e a contagem positiva pode ser bem modelada por uma distribuição de Poisson.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28299748",
   "metadata": {},
   "source": [
    "### Teste de Vuong\n",
    "\n",
    "O **Teste de Vuong** é um teste estatístico utilizado para comparar a qualidade de ajuste de dois modelos aninhados ou não aninhados, que são usados para modelar os mesmos dados. Esse teste é particularmente útil para comparar modelos de contagem, como o **Modelo Zero-Inflated Poisson (ZIP)** e o **Modelo Zero-Inflated Negative Binomial (ZINB)**, ou para decidir entre um modelo de Poisson e um modelo binomial negativo.\n",
    "\n",
    "#### Contexto e Aplicação do Teste de Vuong\n",
    "\n",
    "Quando você tem dois modelos diferentes que podem ser utilizados para modelar os mesmos dados, o Teste de Vuong ajuda a determinar qual modelo se ajusta melhor aos dados. É especialmente útil quando você suspeita que um modelo pode ser superior ao outro, mas ambos os modelos têm boas razões para serem considerados.\n",
    "\n",
    "O teste compara os logaritmos das funções de verossimilhança dos dois modelos, medindo se há uma diferença significativa entre os dois em termos de ajuste aos dados.\n",
    "\n",
    "#### Fórmula do Teste de Vuong\n",
    "\n",
    "O Teste de Vuong é baseado na estatística de Vuong, que é calculada como:\n",
    "\n",
    "$$\n",
    "V = \\frac{\\sqrt{n} \\cdot \\bar{m}}{s_m}\n",
    "$$\n",
    "\n",
    "Onde:\n",
    "- $n$ é o número de observações.\n",
    "- $\\bar{m}$ é a média das diferenças entre os logaritmos das funções de verossimilhança dos dois modelos para cada observação.\n",
    "- $s_m$ é o desvio padrão dessas diferenças.\n",
    "\n",
    "A fórmula para $\\bar{m}$ é:\n",
    "\n",
    "$$\n",
    "\\bar{m} = \\frac{1}{n} \\sum_{i=1}^{n} ( \\ln \\mathcal{L}_1(y_i | x_i) - \\ln \\mathcal{L}_2(y_i | x_i) )\n",
    "$$\n",
    "\n",
    "Onde:\n",
    "- $\\mathcal{L}_1$ e $\\mathcal{L}_2$ são as funções de verossimilhança dos dois modelos comparados.\n",
    "- $y_i$ é a observação i.\n",
    "- $x_i$ são as covariáveis associadas à observação $y_i$.\n",
    "\n",
    "#### Interpretação do Teste de Vuong\n",
    "\n",
    "A estatística $V$ segue uma distribuição normal padrão sob a hipótese nula de que ambos os modelos têm o mesmo poder preditivo.\n",
    "\n",
    "- **Se $V > 1.96$**: O primeiro modelo (modelo 1) se ajusta significativamente melhor aos dados do que o segundo modelo (modelo 2), ao nível de significância de 5%.\n",
    "- **Se $V < -1.96$**: O segundo modelo (modelo 2) se ajusta significativamente melhor aos dados do que o primeiro modelo (modelo 1), ao nível de significância de 5%.\n",
    "- **Se $-1.96 \\leq V \\leq 1.96$**: Não há evidência suficiente para preferir um modelo ao outro; ambos os modelos têm desempenhos semelhantes em termos de ajuste aos dados.\n",
    "\n",
    "#### Aplicações Práticas\n",
    "\n",
    "O Teste de Vuong é frequentemente usado nas seguintes situações:\n",
    "\n",
    "1. **Comparação entre Modelos Zero-Inflated**: Comparar o modelo ZIP com o ZINB para determinar qual modelo lida melhor com os zeros inflacionados e a sobredispersão.\n",
    "2. **Decisão entre Modelos de Contagem**: Comparar um modelo de Poisson com um modelo binomial negativo para verificar qual modelo descreve melhor os dados de contagem.\n",
    "3. **Comparação de Modelos Aninhados e Não Aninhados**: Comparar qualquer par de modelos (aninhados ou não aninhados) para determinar qual tem melhor qualidade de ajuste.\n",
    "\n",
    "#### Limitações do Teste de Vuong\n",
    "\n",
    "- **Sensibilidade à Especificação do Modelo**: O teste é sensível à especificação dos modelos, e resultados divergentes podem ocorrer se os modelos não forem especificados corretamente.\n",
    "- **Dependência de Assumptions**: O teste assume que as observações são independentes e identicamente distribuídas (iid). Se essa suposição for violada, os resultados do teste podem ser comprometidos.\n",
    "- **Comparação Direta**: O teste só pode comparar dois modelos de cada vez, o que pode ser uma limitação em contextos onde vários modelos precisam ser comparados simultaneamente.\n",
    "\n",
    "#### Exemplo de Uso\n",
    "\n",
    "Suponha que você tenha ajustado um modelo ZIP e um modelo ZINB aos mesmos dados e deseja determinar qual modelo se ajusta melhor. O Teste de Vuong pode ser utilizado para comparar diretamente os logaritmos das funções de verossimilhança de ambos os modelos, fornecendo evidências sobre qual modelo é superior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ed92eb",
   "metadata": {},
   "source": [
    "### Aplicação em Python utilizando o rpy2 (R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef689d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Call:\n",
      "zeroinfl(formula = Y ~ medu + fedu + mage + deadkids + alcohol, data = rdf, \n",
      "    dist = \"negbin\")\n",
      "\n",
      "Pearson residuals:\n",
      "     Min       1Q   Median       3Q      Max \n",
      "-3.43479 -0.53576  0.06026  0.54287  7.11062 \n",
      "\n",
      "Count model coefficients (negbin with log link):\n",
      "              Estimate Std. Error z value Pr(>|z|)    \n",
      "(Intercept)  1.9923310  0.0274035  72.704  < 2e-16 ***\n",
      "medu         0.0146850  0.0023345   6.290 3.17e-10 ***\n",
      "fedu         0.0089467  0.0015902   5.626 1.84e-08 ***\n",
      "mage         0.0040253  0.0009114   4.417 1.00e-05 ***\n",
      "deadkids    -0.0178692  0.0104653  -1.707  0.08773 .  \n",
      "alcohol     -0.0464488  0.0270871  -1.715  0.08638 .  \n",
      "Log(theta)   6.9966781  2.1370895   3.274  0.00106 ** \n",
      "\n",
      "Zero-inflation model coefficients (binomial with logit link):\n",
      "            Estimate Std. Error z value Pr(>|z|)    \n",
      "(Intercept)  0.03559    0.59479   0.060    0.952    \n",
      "medu        -0.05506    0.04107  -1.341    0.180    \n",
      "fedu        -0.11754    0.02384  -4.930 8.23e-07 ***\n",
      "mage        -0.09479    0.02383  -3.979 6.93e-05 ***\n",
      "deadkids     0.24225    0.26562   0.912    0.362    \n",
      "alcohol      1.71204    0.33271   5.146 2.66e-07 ***\n",
      "---\n",
      "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n",
      "\n",
      "Theta = 1092.9963 \n",
      "Number of iterations in BFGS optimization: 44 \n",
      "Log-likelihood: -1.239e+04 on 13 Df\n",
      "\n",
      "\n",
      "Call:\n",
      "zeroinfl(formula = Y ~ medu + fedu + mage + deadkids + alcohol, data = rdf, \n",
      "    dist = \"poisson\")\n",
      "\n",
      "Pearson residuals:\n",
      "     Min       1Q   Median       3Q      Max \n",
      "-3.45348 -0.53825  0.06053  0.54493  7.11798 \n",
      "\n",
      "Count model coefficients (poisson with log link):\n",
      "              Estimate Std. Error z value Pr(>|z|)    \n",
      "(Intercept)  1.9924487  0.0272652  73.077  < 2e-16 ***\n",
      "medu         0.0146773  0.0023232   6.318 2.65e-10 ***\n",
      "fedu         0.0089470  0.0015829   5.652 1.58e-08 ***\n",
      "mage         0.0040245  0.0009068   4.438 9.08e-06 ***\n",
      "deadkids    -0.0178686  0.0104130  -1.716   0.0862 .  \n",
      "alcohol     -0.0464841  0.0269579  -1.724   0.0847 .  \n",
      "\n",
      "Zero-inflation model coefficients (binomial with logit link):\n",
      "            Estimate Std. Error z value Pr(>|z|)    \n",
      "(Intercept)  0.03565    0.59475   0.060    0.952    \n",
      "medu        -0.05507    0.04106  -1.341    0.180    \n",
      "fedu        -0.11754    0.02384  -4.930 8.23e-07 ***\n",
      "mage        -0.09479    0.02382  -3.979 6.93e-05 ***\n",
      "deadkids     0.24223    0.26560   0.912    0.362    \n",
      "alcohol      1.71199    0.33269   5.146 2.66e-07 ***\n",
      "---\n",
      "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n",
      "\n",
      "Number of iterations in BFGS optimization: 18 \n",
      "Log-likelihood: -1.239e+04 on 12 Df\n",
      "\n",
      "Vuong Non-Nested Hypothesis Test-Statistic: \n",
      "(test-statistic is asymptotically distributed N(0,1) under the\n",
      " null that the models are indistinguishible)\n",
      "-------------------------------------------------------------\n",
      "              Vuong z-statistic             H_A p-value\n",
      "Raw                   0.1398418 model1 > model2 0.44439\n",
      "AIC-corrected         0.1398418 model1 > model2 0.44439\n",
      "BIC-corrected         0.1398418 model1 > model2 0.44439\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rpy2 import robjects\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.packages import importr\n",
    "\n",
    "# Ativar a conversão automática entre pandas e R\n",
    "pandas2ri.activate()\n",
    "\n",
    "# Carregar o pacote R 'pscl'\n",
    "pscl = importr('pscl')\n",
    "\n",
    "# Carregar os dados no Python\n",
    "df = pd.read_stata(\"https://github.com/Daniel-Uhr/data/raw/main/cattaneo2.dta\")\n",
    "df['Y'] = df['nprenatal']\n",
    "\n",
    "# Passar o DataFrame pandas para um DataFrame R\n",
    "rdf = pandas2ri.py2rpy(df)\n",
    "\n",
    "# Colocar o DataFrame R no ambiente global do R\n",
    "robjects.globalenv['rdf'] = rdf\n",
    "\n",
    "# Definir a fórmula do modelo\n",
    "formula = 'Y ~ medu + fedu + mage + deadkids + alcohol'\n",
    "\n",
    "# Ajustar o modelo ZINB usando o pacote pscl\n",
    "robjects.r(f'''\n",
    "zinb_model <- zeroinfl({formula}, data=rdf, dist=\"negbin\")\n",
    "''')\n",
    "# Obter o resumo do modelo ZINB\n",
    "print(robjects.r('summary(zinb_model)'))\n",
    "\n",
    "# Ajustar o modelo ZIP usando o pacote pscl\n",
    "robjects.r(f'''\n",
    "zip_model <- zeroinfl({formula}, data=rdf, dist=\"poisson\")\n",
    "''')\n",
    "# Obter o resumo do modelo ZIP\n",
    "print(robjects.r('summary(zip_model)'))\n",
    "\n",
    "# Realizar o Teste de Vuong diretamente em R\n",
    "vuong_test = robjects.r('vuong(zinb_model, zip_model)')\n",
    "\n",
    "# Obter o resumo do Teste de Vuong\n",
    "print(vuong_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d07587",
   "metadata": {},
   "source": [
    "Dado o p-valor de 0.44439, que é bem acima do nível de significância comum (0.05), não há evidência estatística para preferir o modelo ZINB ao modelo ZIP. Em outras palavras, ambos os modelos parecem ajustar os dados de maneira semelhante, e não há uma vantagem clara em usar um sobre o outro com base no Teste de Vuong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80ad2a2",
   "metadata": {},
   "source": [
    "### Modelo Tobit\n",
    "\n",
    "O **modelo Tobit** é utilizado em situações onde a variável dependente de uma regressão linear é censurada, ou seja, só é observada dentro de um intervalo específico. Esse modelo é particularmente útil quando há um limite superior ou inferior além do qual as observações não são registradas ou são reportadas como um valor fixo, como zero.\n",
    "\n",
    "#### Cenário de Aplicação\n",
    "\n",
    "Em um conjunto de dados $(y_i, x_i)$, $i=1, \\ldots, N$, as variáveis explicativas $x_i$ são completamente observadas e exógenas, mas a variável dependente $y_i$ pode não ser completamente observada. Por exemplo, imagine um estudo sobre os gastos domiciliares com determinado bem. Se alguns domicílios não compram o bem, o gasto observado será zero. Neste caso, $y_i$ pode ser tratado como censurado.\n",
    "\n",
    "Nesse contexto, podemos interpretar que existe uma variável latente $y^*$ que representa a verdadeira demanda por esse bem. No entanto, essa demanda só se manifesta quando ultrapassa um certo limite, $L$. Se a demanda for menor ou igual a esse limite, observamos um valor censurado.\n",
    "\n",
    "#### Estrutura do Modelo Tobit\n",
    "\n",
    "A regressão Tobit é especificada em termos de uma variável latente $y_i^*$, que segue a seguinte forma:\n",
    "\n",
    "$$ y_i^* = x_i' \\beta + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma^2) $$\n",
    "\n",
    "Onde:\n",
    "- $y_i^*$ é a variável latente (não observada diretamente).\n",
    "- $x_i$ é o vetor de variáveis explicativas.\n",
    "- $\\beta$ é o vetor de coeficientes a serem estimados.\n",
    "- $\\epsilon_i$ são os erros, assumidos como normalmente distribuídos com média zero e variância constante $\\sigma^2$.\n",
    "\n",
    "A variável observada $y_i$ está relacionada à variável latente $y_i^*$ da seguinte forma:\n",
    "\n",
    "- **Censura à Esquerda**: \n",
    "  - $y_i = y_i^*$ se $y_i^* > L$\n",
    "  - $y_i = L$ se $y_i^* \\leq L$\n",
    "  \n",
    "  Aqui, $L$ é o ponto de censura inferior.\n",
    "\n",
    "- **Censura à Direita**: \n",
    "  - $y_i = y_i^*$ se $y_i^* < U$\n",
    "  - $y_i = U$ se $y_i^* \\geq U$\n",
    "  \n",
    "  Aqui, $U$ é o ponto de censura superior.\n",
    "\n",
    "#### Função de Verossimilhança\n",
    "\n",
    "A função de verossimilhança para o modelo Tobit combina as probabilidades associadas às observações censuradas e não censuradas. Para uma censura à esquerda:\n",
    "\n",
    "- Para as observações não censuradas ($y_i > L$), a contribuição para a verossimilhança é a densidade normal:\n",
    "  \n",
    "  $$ f(y_i | x_i) = \\frac{1}{\\sigma} \\phi\\left(\\frac{y_i - x_i' \\beta}{\\sigma}\\right) $$\n",
    "\n",
    "  Onde $\\phi(.)$ é a função densidade da distribuição normal padrão.\n",
    "\n",
    "- Para as observações censuradas ($y_i = L$), a contribuição para a verossimilhança é a probabilidade acumulada:\n",
    "\n",
    "  $$ F(L | x_i) = \\Phi\\left(\\frac{L - x_i' \\beta}{\\sigma}\\right) $$\n",
    "\n",
    "  Onde $\\Phi(.)$ é a função de distribuição acumulada da normal padrão.\n",
    "\n",
    "A função de verossimilhança total é a combinação dos termos para todas as observações.\n",
    "\n",
    "#### Considerações sobre o Modelo Tobit\n",
    "\n",
    "O modelo Tobit pressupõe normalidade dos erros e homocedasticidade. No entanto, em algumas aplicações, como em dados de consumo, pode ser mais adequado modelar os dados como lognormais, especialmente se os valores observados forem sempre positivos:\n",
    "\n",
    "$$ y_i^* = \\exp(x_i' \\beta + \\epsilon_i), \\quad \\epsilon_i \\sim N(0, \\sigma^2) $$\n",
    "\n",
    "Nesse caso, a variável observada $y_i$ é dada por:\n",
    "\n",
    "- $y_i = y_i^*$ se $\\ln y_i^* > \\gamma$\n",
    "- $y_i = 0$ se $\\ln y_i^* \\leq \\gamma$\n",
    "\n",
    "Aqui, $\\gamma$ é o ponto de censura em termos do logaritmo da variável latente.\n",
    "\n",
    "#### Diagnóstico e Validade do Modelo Tobit\n",
    "\n",
    "A consistência do estimador Tobit depende das seguintes condições:\n",
    "\n",
    "1. **Normalidade dos Erros**: A suposição de normalidade pode ser verificada usando testes como o `sktest`. Se os erros não forem normalmente distribuídos, as estimativas podem ser inconsistentes.\n",
    "\n",
    "2. **Homocedasticidade**: A suposição de variância constante (homocedasticidade) é essencial para a consistência do modelo. Isso pode ser testado usando o teste de Lagrange Multiplier (LM). Se a homocedasticidade não for verificada, o modelo pode produzir estimativas enviesadas.\n",
    "\n",
    "Mesmo que o modelo Tobit produza estimativas que parecem razoáveis, é crucial realizar diagnósticos rigorosos para garantir que as suposições subjacentes sejam válidas.\n",
    "\n",
    "#### Alternativas ao Modelo Tobit\n",
    "\n",
    "Caso as suposições do modelo Tobit sejam violadas, é importante considerar alternativas:\n",
    "\n",
    "- **Modelo Truncado**: Utilizado quando as observações abaixo (ou acima) de um certo valor são completamente excluídas do conjunto de dados, ao invés de serem censuradas.\n",
    "\n",
    "- **Modelo de Heckman**: Usado para lidar com problemas de seleção amostral. Esse modelo é útil quando a decisão de observar ou não a variável dependente está correlacionada com fatores que também afetam o resultado. Ele corrige o viés de seleção e fornece estimativas consistentes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555689ba",
   "metadata": {},
   "source": [
    "### Tobit\n",
    "\n",
    "O modelo Tobit é relevante quando a variável dependente de uma regressão linear é observada somente para algum intervalo. Os dados $(y_i, x_i)$, $i=1, \\ldots, N$ assumem que $x_i$ é completamente observado (exógeno), mas $y_i$ nem sempre é observado. Ou seja, alguns $y_i$ são zero.\n",
    "\n",
    "Uma interpretação é que os valores zero são observações censuradas (no sentido de que há perda de informação, podem ser “missings”). Por exemplo, considere uma variável latente (não observável) domiciliar, $y^*$, por demanda de bens. Ela não é expressa até que seja feita a compra em determinado nível, denotado por $L$, seja ultrapassado. Assim, $L$ pode adotar um valor maior que zero, mesmo que desconhecido.\n",
    "\n",
    "A distribuição tem suporte sobre o intervalo de $-\\infty$ a $+\\infty$, mas somente observamos valores no intervalo $[L, U]$, ou seja, $L$ é o ponto de corte inferior, e $U$ o ponto de corte superior.\n",
    "\n",
    "#### Estrutura do Modelo Tobit\n",
    "\n",
    "A regressão de interesse é especificada como uma variável latente, $y^*$:\n",
    "\n",
    "$$ y_i^* = x_i' \\beta + \\epsilon_i, \\quad i = 1, \\ldots, N $$\n",
    "\n",
    "Onde $\\epsilon_i \\sim N(0, \\sigma^2)$ e $x_i$ denota o vetor $(K \\times 1)$ exógeno e completamente observado. Se $y_i^*$ fosse observado, estimaríamos por OLS.\n",
    "\n",
    "A variável observável $y_i$ é relacionada à variável latente $y_i^*$ através da regra de observação:\n",
    "\n",
    "$$ y_i = \\begin{cases} \n",
    "y_i^* & \\text{se } y_i^* > L \\\\\n",
    "L & \\text{se } y_i^* \\leq L \n",
    "\\end{cases} $$\n",
    "\n",
    "A probabilidade de uma observação ser censurada é $ \\Pr(y_i^* \\leq L) = \\Pr(x_i' \\beta + \\epsilon_i \\leq L) = \\Phi \\left(\\frac{L - x_i' \\beta}{\\sigma} \\right)$, onde $\\Phi(.)$ é a Função de Distribuição Acumulada Normal Padrão.\n",
    "\n",
    "A censura também pode ocorrer pela direita:\n",
    "\n",
    "$$ y_i = \\begin{cases} \n",
    "y_i^* & \\text{se } y_i^* < U \\\\\n",
    "U & \\text{se } y_i^* \\geq U \n",
    "\\end{cases} $$\n",
    "\n",
    "#### Considerações sobre o Modelo Tobit\n",
    "\n",
    "Modelos Tobit assumem normalidade. Entretanto, dados de consumo são melhores modelados como lognormal. Complicações adicionais: threshold não zero e $y$ lognormal. Então:\n",
    "\n",
    "$$ y_i^* = \\exp(x_i' \\beta + \\epsilon_i), \\quad \\epsilon_i \\sim N(0, \\sigma^2) $$\n",
    "\n",
    "Onde observamos que:\n",
    "\n",
    "$$ y_i = \\begin{cases} \n",
    "y_i^* & \\text{se } \\ln y_i^* > \\gamma \\\\\n",
    "0 & \\text{se } \\ln y_i^* \\leq \\gamma \n",
    "\\end{cases} $$\n",
    "\n",
    "Aqui se conhece que $y_i = 0$ quando os dados são censurados, e em geral $\\gamma \\ne 0$. Podemos rodar o Tobit com a opção `ll(#)`, onde a variável dependente é $\\ln y$, e o valor do threshold # é igual ao valor mínimo não censurado de $\\ln y$.\n",
    "\n",
    "Ocorre que muitos valores da variável dependente originalmente são zeros, e ao aplicarmos o $\\ln(.)$ teremos dados missing. Além disso, o menor valor pode ser 1, onde $\\ln(1)$ é zero. E `ll(0)` tratará a observação como censurada.\n",
    "\n",
    "#### Validade do Tobit\n",
    "\n",
    "A consistência do estimador Tobit depende de:\n",
    "- Especificação Correta do Modelo\n",
    "- Exogeneidade dos Regressores\n",
    "- **Normalidade**\n",
    "- **Homocedasticidade**\n",
    "  \n",
    "Apesar das estimações aparentemente satisfatórias, os diagnósticos revelam a fraqueza do modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83b589a",
   "metadata": {},
   "source": [
    "Exemplo Aplicado Tobit: Limite de censura constante\n",
    "\n",
    "Os administradores universitários querem saber a relação entre a média das notas do ensino médio (GPA) e o desempenho dos alunos na faculdade. O arquivo gpa.dta contém dados fictícios sobre uma coorte de 4.000 estudantes universitários. O GPA da faculdade (gpa2) e o GPA do ensino médio (hsgpa) são medidos em uma escala contínua entre zero e quatro. O resultado de interesse é o GPA universitário do estudante. Porém, por razões de confidencialidade, os GPAs abaixo de 2.0 são reportados como 2.0. Em outras palavras, o resultado é censurado à esquerda. Acreditamos que o GPA também é uma função do logaritmo da renda dos pais do estudante (pincome) e se o estudante participou ou não de um programa de habilidades de estudo durante a faculdade (program)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "693405d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sys\n",
    "# URL do arquivo tobit.py\n",
    "url = 'https://raw.githubusercontent.com/Daniel-Uhr/estimators/main/tobit.py'\n",
    "\n",
    "# Fazer o download do arquivo\n",
    "response = requests.get(url)\n",
    "code = response.text\n",
    "# Executar o código do arquivo baixado\n",
    "exec(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f9b65ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    4000.000000\n",
      "mean        2.571925\n",
      "std         0.585179\n",
      "min         2.000000\n",
      "25%         2.000000\n",
      "50%         2.426000\n",
      "75%         2.981000\n",
      "max         4.000000\n",
      "Name: gpa2, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Carregar os dados\n",
    "df = pd.read_stata(\"https://github.com/Daniel-Uhr/data/raw/main/gpa.dta\")\n",
    "\n",
    "# Preparar as variáveis independentes e a variável dependente censurada\n",
    "x = df[['hsgpa', 'pincome', 'program']]\n",
    "\n",
    "# Converter a variável categórica 'program' em variáveis dummy\n",
    "x = pd.get_dummies(x, drop_first=True)\n",
    "\n",
    "y = df['gpa2']\n",
    "\n",
    "# sumarizar a variável y\n",
    "print(y.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eee3940c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Tobit Regression Results\n",
      "========================================\n",
      "Dep. Variable:                     gpa2\n",
      "Model:                            Tobit\n",
      "Method:                           Maximum Likelihood\n",
      "Date:                             dom, 11 ago 2024\n",
      "Time:                             18:57:33\n",
      "No. Observations:                 4000\n",
      "No. Censored Observations:        1310\n",
      "No. Uncensored Observations:      2690\n",
      "==========================================================================\n",
      "                 coef   std err          z  P>|z|    [0.025    0.975]\n",
      "const        0.681005  0.049097  13.870657    0.0  0.584775  0.777234\n",
      "hsgpa        0.681005  0.012887  52.844776    0.0  0.655746  0.706263\n",
      "pincome      0.329633  0.007449  44.254492    0.0  0.315034  0.344232\n",
      "program_Yes  0.571642  0.015109  37.835526    0.0  0.542029  0.601255\n",
      "==========================================================================\n",
      "Sigma (scale):                    0.4151\n",
      "Log-likelihood:                   2134.6044\n",
      "Number of Iterations:             15\n",
      "==========================================================================\n",
      "Mean Absolute Error: 0.8694328578311515\n"
     ]
    }
   ],
   "source": [
    "# Considerando censura superior em 4.0 e inferior em 2.0\n",
    "cens = pd.Series(0, index=df.index)\n",
    "cens[y >= 4.0] = 1  # Censura à direita\n",
    "cens[y <= 2.0] = -1  # Censura à esquerda\n",
    "\n",
    "\n",
    "# Criar e ajustar o modelo Tobit\n",
    "model = TobitModel(fit_intercept=True)\n",
    "model.fit(x, y, cens)\n",
    "\n",
    "# As previsões e a avaliação do modelo são opcionais\n",
    "y_pred = model.predict(x)\n",
    "mae = model.score(x, y, scoring_function=mean_absolute_error)\n",
    "print(\"Mean Absolute Error:\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7141cbf2",
   "metadata": {},
   "source": [
    "Com base nas estatísticas descritivas, é razoável considerar a censura superior em 4.0, mas a censura inferior em 2.0 dependerá do contexto e de um entendimento adicional dos dados. Se há um conhecimento prévio ou uma razão lógica para acreditar que 2.0 é um limite inferior, então você deve configurar o modelo para considerar isso. Caso contrário, a censura inferior pode não ser necessária.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff9624e",
   "metadata": {},
   "source": [
    "A regressão Tobit relata os coeficientes para o modelo de regressão latente. Assim, podemos interpretar os coeficientes da mesma forma que interpretaríamos os coeficientes de uma regressão OLS. Por exemplo, a participação em um programa de habilidades de estudo aumenta a expectativa do GPA não censurado em 0,56 pontos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aaa20469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapiro-Wilk Test:\n",
      "Statistic: 0.9982970356941223, p-value: 0.00026581567362882197\n",
      "Breusch-Pagan Test:\n",
      "Lagrange multiplier statistic: 1021.5150332114224, p-value: 3.7820086335399504e-224\n",
      "F-statistic: 1371.1726425742825, F-test p-value: 2.488799594838788e-258\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "\n",
    "# Supondo que 'model' é o seu objeto TobitModel ajustado\n",
    "# e que 'y_pred' são as previsões do modelo.\n",
    "\n",
    "# 1. Resíduos do modelo\n",
    "residuals = y - y_pred\n",
    "\n",
    "# 2. Teste de Shapiro-Wilk para Normalidade\n",
    "shapiro_test = stats.shapiro(residuals)\n",
    "print(\"Shapiro-Wilk Test:\")\n",
    "print(f\"Statistic: {shapiro_test.statistic}, p-value: {shapiro_test.pvalue}\")\n",
    "\n",
    "# 3. Teste de Breusch-Pagan para Homocedasticidade\n",
    "# Adicionando uma constante aos valores ajustados\n",
    "exog_het = sm.add_constant(y_pred)\n",
    "bp_test = het_breuschpagan(residuals, exog_het)\n",
    "print(\"Breusch-Pagan Test:\")\n",
    "print(f\"Lagrange multiplier statistic: {bp_test[0]}, p-value: {bp_test[1]}\")\n",
    "print(f\"F-statistic: {bp_test[2]}, F-test p-value: {bp_test[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd84344",
   "metadata": {},
   "source": [
    "os testes de Shapiro-Wilk e Breusch-Pagan indicam que os pressupostos de normalidade dos resíduos e homocedasticidade dos resíduos foram violados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220ea895",
   "metadata": {},
   "source": [
    "### Modelo de Seleção de Heckman\n",
    "\n",
    "O **Modelo de Seleção de Heckman** é uma abordagem para corrigir o viés de seleção amostral que ocorre quando a amostra observada não é aleatória, mas sim selecionada de acordo com alguma regra ou decisão, o que pode gerar resultados enviesados se não for tratado adequadamente. O modelo é amplamente utilizado em situações onde a decisão de participar em um evento ou a decisão de consumo pode estar correlacionada com o volume do resultado observado.\n",
    "\n",
    "#### Cenário do Modelo\n",
    "\n",
    "Considerando o exemplo anterior do modelo Tobit, temos duas decisões que precisam ser modeladas:\n",
    "\n",
    "1. **Decisão de gastar (participação)**: Se o indivíduo decide ou não participar do mercado (por exemplo, decidir se irá gastar ou não).\n",
    "2. **Volume de gastos (resultado)**: Quanto o indivíduo gasta, dado que ele decidiu participar.\n",
    "\n",
    "Se tratarmos essas duas decisões como independentes, podemos cometer um erro, já que a decisão de participar pode influenciar o volume de gastos, criando um viés de seleção amostral.\n",
    "\n",
    "### Viés de Seleção Amostral\n",
    "\n",
    "Mesmo depois de controlar por variáveis explicativas, o nível de consumo positivo observado pode não ser selecionado aleatoriamente da população. Em outras palavras, os indivíduos \"se selecionam\" para participar do mercado de consumo, o que leva ao **viés de seleção amostral**.\n",
    "\n",
    "### Procedimento de Heckman\n",
    "\n",
    "Para corrigir o viés de seleção amostral, utilizamos o **procedimento de Heckman**, que é um modelo de seleção em duas partes:\n",
    "\n",
    "1. **Parte 1: Modelo de Seleção (Probit)**:\n",
    "   - Primeiramente, estimamos a probabilidade de participação utilizando um modelo Probit.\n",
    "   - O modelo Probit estima a probabilidade de um resultado binário (participação ou não).\n",
    "\n",
    "2. **Parte 2: Modelo de Resultado (OLS com Correção)**:\n",
    "   - Em seguida, utilizamos os resultados da primeira parte para ajustar o modelo de resultado.\n",
    "   - O modelo OLS é corrigido pela inclusão de uma estimativa do regressor omitido, conhecido como **razão inversa de Mills**.\n",
    "\n",
    "### Derivação do Modelo\n",
    "\n",
    "Para o caso de truncamento à esquerda, em zero, somente observamos $y$ se $y^* > 0$. Então:\n",
    "\n",
    "$$ E[y] = E[y^* | y^* > 0] $$\n",
    "\n",
    "Podemos decompor isso da seguinte maneira:\n",
    "\n",
    "$$ E[y]= E[x' \\beta + \\epsilon | x' \\beta + \\epsilon > 0] $$\n",
    "\n",
    "$$ E[y]= E[x' \\beta | x' \\beta + \\epsilon > 0] + E[\\epsilon | x' \\beta + \\epsilon > 0] $$\n",
    "\n",
    "$$ E[y]= x' \\beta + E[\\epsilon | \\epsilon > -x' \\beta] $$\n",
    "\n",
    "Para uma variável padronizada, $z \\sim N(0,1)$, truncada à esquerda em zero, tem-se que seu primeiro momento é:\n",
    "\n",
    "$$ E[z | z > -c] = \\frac{\\phi(c)}{\\Phi(c)} $$\n",
    "\n",
    "Então, temos que:\n",
    "\n",
    "$$ E[y] = E[y^* | y^* > 0] = x' \\beta + E[\\epsilon | \\epsilon > -x' \\beta] $$\n",
    "\n",
    "Para o último termo:\n",
    "\n",
    "$$ E[\\epsilon | \\epsilon > -x' \\beta] = \\sigma E\\left[\\frac{\\epsilon}{\\sigma} \\Bigg| \\frac{\\epsilon}{\\sigma} > \\frac{-x' \\beta}{\\sigma} \\right] $$\n",
    "\n",
    "$$ = \\sigma \\frac{\\phi\\left(\\frac{x' \\beta}{\\sigma}\\right)}{\\Phi\\left(\\frac{x' \\beta}{\\sigma}\\right)} $$\n",
    "\n",
    "$$ = \\sigma \\lambda\\left(\\frac{x' \\beta}{\\sigma}\\right) $$\n",
    "\n",
    "Onde $\\lambda(\\cdot)$ é chamada a razão inversa de Mills.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Modelo Heckman de Dois Passos\n",
    "\n",
    "No **procedimento de dois passos de Heckman** (Heckit), corrigimos a regressão OLS adicionando uma estimativa do regressor omitido $ \\lambda(x_1' \\beta_1) $. Usando os valores positivos de $ y_2 $, estimamos pelo modelo OLS:\n",
    "\n",
    "$$ y_{2i} = x_{2i}' \\beta_2 + \\sigma_{12} \\lambda(x_1' \\hat{\\beta_1}) + v_i $$\n",
    "\n",
    "Aqui, $ \\hat{\\beta_1} $ são os coeficientes obtidos no primeiro passo (regressão Probit). Além disso, $ x_1 $ e $ x_2 $ podem ser diferentes, permitindo a inclusão de variáveis que identificam a participação, conhecidas como **variáveis de exclusão**.\n",
    "\n",
    "### Notas Importantes:\n",
    "\n",
    "- **Restrição de Exclusão**: A inclusão de uma variável que afeta a participação (decisão de gastar), mas não afeta diretamente o volume de gastos, é crucial para a identificação do modelo. Isso é conhecido como uma **restrição de exclusão**.\n",
    "- **Renda como Variável de Exclusão**: No exemplo, a renda é utilizada como uma variável de exclusão, mas é importante observar que a renda também pode ser uma variável de resultado, o que pode complicar sua utilização como variável de exclusão.\n",
    "\n",
    "### Considerações Finais\n",
    "\n",
    "O **Modelo de Seleção de Heckman** é uma ferramenta poderosa para lidar com o viés de seleção amostral em econometria. Ele permite modelar de forma mais realista situações onde a decisão de participar em um evento e o resultado desse evento estão inter-relacionados. A correção de Heckman ajuda a evitar a distorção nos coeficientes e a obter estimativas consistentes.\n",
    "\n",
    "### Exercícios e Discussão\n",
    "\n",
    "- **Exercício Prático**: Use um software econométrico, como o Stata, para aplicar o procedimento de Heckman a um conjunto de dados com viés de seleção.\n",
    "- **Discussão**: Discuta a importância da escolha de variáveis de exclusão adequadas e os desafios na identificação do modelo de seleção de Heckman.\n",
    "\n",
    "Essa aula fornece uma base sólida tanto para a compreensão teórica quanto para a aplicação prática do modelo de seleção de Heckman. Se precisar de mais exemplos ou aprofundamento em algum aspecto específico, estou à disposição!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "41e984a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Adicione o caminho do script heckman.py ao Python Path\n",
    "sys.path.append(os.path.join(os.getcwd(), 'estimators'))\n",
    "\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from heckman import Heckman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba603d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_stata(\"https://github.com/Daniel-Uhr/data/raw/main/womenwk.dta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3ac127",
   "metadata": {},
   "source": [
    "Roda somente o two-step Heckman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7dbedc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Heckman Regression Results      \n",
      "=======================================\n",
      "Dep. Variable:                     wage\n",
      "Model:                          Heckman\n",
      "Method:                Heckman Two-Step\n",
      "Date:                  dom, 11 ago 2024\n",
      "Time:                          18:47:11\n",
      "No. Total Obs.:                    2000\n",
      "No. Censored Obs.:                  657\n",
      "No. Uncensored Obs.:               1343\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.7340      1.248      0.588      0.557      -1.713       3.181\n",
      "education      0.9825      0.054     18.235      0.000       0.877       1.088\n",
      "age            0.2119      0.022      9.608      0.000       0.169       0.255\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -2.4674      0.193    -12.813      0.000      -2.845      -2.090\n",
      "married        0.4309      0.074      5.806      0.000       0.285       0.576\n",
      "children       0.4473      0.029     15.564      0.000       0.391       0.504\n",
      "education      0.0584      0.011      5.318      0.000       0.037       0.080\n",
      "age            0.0347      0.004      8.210      0.000       0.026       0.043\n",
      "================================================================================\n",
      "                   coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "IMR (Lambda)     4.0016      0.607      6.597      0.000       2.813       5.190\n",
      "=====================================\n",
      "rho:                            0.673\n",
      "sigma:                          5.947\n",
      "=====================================\n",
      "\n",
      "First table are the estimates for the regression (response) equation.\n",
      "Second table are the estimates for the selection equation.\n",
      "Third table is the estimate for the coef of the inverse Mills ratio (Heckman's Lambda).\n"
     ]
    }
   ],
   "source": [
    "# Definir as variáveis endógenas, exógenas e de seleção\n",
    "endog = df3['wage']\n",
    "exog = sm.add_constant(df3[['education', 'age']])\n",
    "exog_select = sm.add_constant(df3[['married', 'children', 'education', 'age']])\n",
    "\n",
    "# Ajustar o modelo Heckman\n",
    "model = Heckman(endog, exog, exog_select)\n",
    "results = model.fit(method='twostep')\n",
    "\n",
    "# Exibir os resultados\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8345adb6",
   "metadata": {},
   "source": [
    "A tabela 1 - Estimativas da Equação de Resposta (Resultado)\n",
    "* Constante (const): O intercepto da equação de resposta não é significativo, indicando que, quando todas as variáveis explicativas são zero, o salário (wage) esperado não é significativamente diferente de zero.\n",
    "* Education (education): A educação tem um coeficiente positivo e altamente significativo, sugerindo que a educação está positivamente associada ao salário.\n",
    "* Age (age): A idade também tem um coeficiente positivo e significativo, indicando que a idade está positivamente correlacionada com o salário.\n",
    "\n",
    "A tabela 2 - Estimativas da Equação de Seleção (Participação)\n",
    "* Constante (const): O intercepto da equação de seleção é significativo e negativo, o que pode indicar a linha de base para a decisão de participar no mercado de trabalho.\n",
    "* Married (married): O fato de ser casado aumenta significativamente a probabilidade de participar no mercado de trabalho.\n",
    "* Children (children): Ter filhos também aumenta significativamente a probabilidade de participação no mercado de trabalho.\n",
    "* Education (education) e Age (age): Tanto a educação quanto a idade aumentam a probabilidade de participação no mercado de trabalho, com coeficientes significativos.\n",
    "\n",
    "A tabela 3 - Estimativas do Modelo de Heckman\n",
    "* IMR (Lambda): O coeficiente da razão inversa de Mills é significativo, indicando que há viés de seleção amostral presente. A correção de Heckman é, portanto, justificada.\n",
    "* Rho (rho): Representa a correlação entre os erros da equação de seleção e da equação de resultado. Um valor de 0,673 sugere uma correlação substancial entre as duas equações.\n",
    "* Sigma (sigma): Estima o desvio padrão dos erros na equação de resultado. Um sigma maior indica maior variabilidade nos salários observados, dado o viés de seleção.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c485f1e3",
   "metadata": {},
   "source": [
    "Considerações Finais\n",
    "Viés de Seleção Amostral:\n",
    "\n",
    "A significância de Lambda (IMR) sugere que o modelo de Heckman corrigiu adequadamente o viés de seleção presente na amostra.\n",
    "Interpretação Prática:\n",
    "\n",
    "O modelo mostra que tanto a decisão de trabalhar quanto o salário são influenciados por variáveis como educação, idade, estado civil e presença de filhos.\n",
    "As duas equações (seleção e resultado) não são independentes, o que é capturado pelo valor de rho.\n",
    "Ajuste de Modelos e Aplicações:\n",
    "\n",
    "O modelo de Heckman é ideal em situações onde a amostra observada não é representativa da população total devido à seleção amostral, como no caso das decisões de participação no mercado de trabalho.\n",
    "Esse exemplo mostra como o modelo de Heckman pode ser aplicado para lidar com o viés de seleção amostral em dados reais. Se precisar de mais detalhes ou exemplos adicionais, estou à disposição!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
